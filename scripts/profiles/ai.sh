#!/bin/bash

# AI profile deployment for NextFlow Docker
# Author: Augment Agent
# Version: 1.0
#
# Services included:
# - Ollama (CPU/GPU) - Local AI models
# - Open WebUI - AI chat interface
# - Cloudflare Tunnel AI - External access

# Source utilities
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"

# Load environment variables
if [[ -f "$PROJECT_ROOT/.env" ]]; then
    source "$PROJECT_ROOT/.env"
else
    echo "âŒ File .env khÃ´ng tá»“n táº¡i táº¡i: $PROJECT_ROOT/.env"
    exit 1
fi

source "$SCRIPT_DIR/../utils/logging.sh"
source "$SCRIPT_DIR/../utils/docker.sh"
source "$SCRIPT_DIR/../utils/validation.sh"

# Profile configuration
PROFILE_NAME="ai"
COMPOSE_FILE="${COMPOSE_FILE:-docker-compose.yml}"

# Services in AI profile - CHá»ˆ AI SERVICES
AI_SERVICES=(
    "ollama"           # Ollama server (GPU-enabled)
    "open-webui"       # AI chat interface
)

# Cloudflare tunnel - Sá»¬ Dá»¤NG TUNNEL CHÃNH (khÃ´ng pháº£i cloudflare-tunnel-ai)
AI_TUNNEL_SERVICE="cloudflare-tunnel"

# Required ports for AI profile
AI_PORTS=(
    "11434:Ollama"
    "5080:Open WebUI"
)

# Deploy AI profile
deploy_ai() {
    local force_deploy="${1:-false}"

    show_banner "TRIá»‚N KHAI PROFILE AI"

    # Show services to be deployed
    log_info "Profile AI bao gá»“m cÃ¡c dá»‹ch vá»¥:"
    for service in "${AI_SERVICES[@]}"; do
        log_info "  âœ“ $service"
    done
    echo

    # Validate requirements
    if ! validate_profile_requirements "$PROFILE_NAME"; then
        log_warning "Má»™t sá»‘ yÃªu cáº§u khÃ´ng Ä‘Æ°á»£c Ä‘Ã¡p á»©ng, nhÆ°ng sáº½ tiáº¿p tá»¥c triá»ƒn khai..."
    fi

    # Kiá»ƒm tra AI containers hiá»‡n táº¡i
    check_existing_ai_containers

    # Create AI directories
    create_ai_directories

    # Deploy AI services
    deploy_ai_services

    # Wait for services to be ready
    wait_for_ai_services

    # Deploy Cloudflare tunnel if configured
    deploy_ai_tunnel

    # Show deployment summary
    show_ai_summary
}

# Kiá»ƒm tra AI containers Ä‘ang cháº¡y Ä‘á»ƒ trÃ¡nh dá»«ng khÃ´ng cáº§n thiáº¿t
check_existing_ai_containers() {
    log_info "ğŸ” Kiá»ƒm tra AI containers Ä‘ang cháº¡y..."

    local running_containers=()
    local containers_to_deploy=()

    # Kiá»ƒm tra AI services
    for service in "${AI_SERVICES[@]}"; do
        if is_container_running "$service"; then
            running_containers+=("$service")
            log_info "âœ… $service Ä‘ang cháº¡y - sáº½ Ä‘Æ°á»£c giá»¯ nguyÃªn vÃ  apply cáº¥u hÃ¬nh má»›i"
        else
            containers_to_deploy+=("$service")
            log_info "ğŸ“¦ $service chÆ°a cháº¡y - sáº½ Ä‘Æ°á»£c triá»ƒn khai"
        fi
    done

    # ThÃ´ng bÃ¡o tá»‘i Æ°u
    if [[ ${#running_containers[@]} -gt 0 ]]; then
        log_success "ğŸ¯ Tá»‘i Æ°u: Giá»¯ nguyÃªn ${#running_containers[@]} AI containers Ä‘ang cháº¡y: ${running_containers[*]}"
    fi

    if [[ ${#containers_to_deploy[@]} -gt 0 ]]; then
        log_info "ğŸ“¦ Sáº½ triá»ƒn khai ${#containers_to_deploy[@]} containers: ${containers_to_deploy[*]}"
    else
        log_success "ğŸ‰ Táº¥t cáº£ AI services Ä‘Ã£ Ä‘ang cháº¡y!"
        log_info "ï¿½ Váº«n sáº½ apply cáº¥u hÃ¬nh má»›i cho táº¥t cáº£ services..."
    fi
}

# Deploy AI services - FUNCTION CHÃNH
deploy_ai_services() {
    log_info "ğŸš€ Triá»ƒn khai AI services..."

    # Kiá»ƒm tra GPU support
    local use_gpu=false
    if docker exec ollama nvidia-smi >/dev/null 2>&1; then
        log_success "âœ… GPU support detected - sá»­ dá»¥ng Ollama GPU"
        use_gpu=true
    else
        log_info "â„¹ï¸ KhÃ´ng cÃ³ GPU support - sá»­ dá»¥ng Ollama CPU"
        use_gpu=false
    fi

    # Deploy Ollama (GPU hoáº·c CPU)
    if [[ "$use_gpu" == "true" ]]; then
        log_loading "Khá»Ÿi Ä‘á»™ng/cáº­p nháº­t Ollama (GPU)..."
        $DOCKER_COMPOSE -f "$COMPOSE_FILE" --profile gpu-nvidia up -d ollama-gpu ollama-pull-llama-gpu
    else
        log_loading "Khá»Ÿi Ä‘á»™ng/cáº­p nháº­t Ollama (CPU)..."
        $DOCKER_COMPOSE -f "$COMPOSE_FILE" --profile cpu up -d ollama-cpu ollama-pull-llama-cpu
    fi

    # Äá»£i Ollama sáºµn sÃ ng
    log_info "â³ Äá»£i Ollama khá»Ÿi Ä‘á»™ng..."
    wait_for_container_health "ollama" 60

    # Deploy Open WebUI
    log_loading "Khá»Ÿi Ä‘á»™ng/cáº­p nháº­t Open WebUI..."
    $DOCKER_COMPOSE -f "$COMPOSE_FILE" up -d open-webui

    log_success "âœ… ÄÃ£ triá»ƒn khai AI services!"
}

# Loáº¡i bá» function check_basic_services_for_ai vÃ¬ AI profile chá»‰ cáº§n Ollama vÃ  Open-WebUI

# Create AI directories
create_ai_directories() {
    log_info "Táº¡o cÃ¡c thÆ° má»¥c AI..."

    local directories=(
        "ollama/models"
        "open-webui/data"
    )

    for dir in "${directories[@]}"; do
        if [[ ! -d "$dir" ]]; then
            mkdir -p "$dir"
            log_debug "Táº¡o thÆ° má»¥c: $dir"
        fi
    done

    log_success "ÄÃ£ táº¡o cÃ¡c thÆ° má»¥c AI!"
}

# Deploy Cloudflare tunnel for AI profile
deploy_ai_tunnel() {
    log_info "ğŸŒ Triá»ƒn khai Cloudflare tunnel cho AI services..."

    # Sá»­ dá»¥ng cloudflare-tunnel chÃ­nh (khÃ´ng pháº£i cloudflare-tunnel-ai)
    if is_container_running "$AI_TUNNEL_SERVICE"; then
        log_info "ğŸ”„ Restart $AI_TUNNEL_SERVICE Ä‘á»ƒ Ã¡p dá»¥ng cáº¥u hÃ¬nh má»›i..."
        docker restart "$AI_TUNNEL_SERVICE"
    else
        log_info "ğŸš€ Khá»Ÿi Ä‘á»™ng $AI_TUNNEL_SERVICE..."
        if ! $DOCKER_COMPOSE -f "$COMPOSE_FILE" up -d "$AI_TUNNEL_SERVICE" --no-deps; then
            log_warning "âš ï¸ KhÃ´ng thá»ƒ khá»Ÿi Ä‘á»™ng $AI_TUNNEL_SERVICE (cÃ³ thá»ƒ do thiáº¿u credentials)"
            log_info "ğŸ’¡ Äá»ƒ cáº¥u hÃ¬nh Cloudflare tunnel:"
            log_info "  1. Kiá»ƒm tra credentials trong cloudflared/credentials/"
            log_info "  2. Cáº­p nháº­t config trong cloudflared/config/cloudflared-config.yml"
            return 0
        fi
    fi

    # Chá» tunnel káº¿t ná»‘i
    log_info "â³ Chá» Cloudflare tunnel káº¿t ná»‘i..."
    sleep 10

    # Kiá»ƒm tra logs Ä‘á»ƒ Ä‘áº£m báº£o tunnel hoáº¡t Ä‘á»™ng
    if docker logs "$AI_TUNNEL_SERVICE" 2>&1 | grep -q -E "(Connection.*registered|connected to cloudflare)"; then
        log_success "âœ… Cloudflare tunnel Ä‘Ã£ káº¿t ná»‘i thÃ nh cÃ´ng!"

        # Hiá»ƒn thá»‹ thÃ´ng tin domains
        log_info "ğŸŒ AI services cÃ³ sáºµn qua Cloudflare tunnel:"
        echo "  ğŸ¤– Open-WebUI: https://open-webui.nextflow.vn"
        echo "  ğŸ§  Ollama API: https://ollama.nextflow.vn"

        return 0
    else
        log_warning "âš ï¸ Cloudflare tunnel Ä‘ang cháº¡y nhÆ°ng cÃ³ thá»ƒ chÆ°a káº¿t ná»‘i hoÃ n toÃ n"
        log_info "ğŸ“‹ Kiá»ƒm tra logs: docker logs $AI_TUNNEL_SERVICE"
        log_info "ğŸ”§ Kiá»ƒm tra cáº¥u hÃ¬nh: cloudflared/config/cloudflared-config.yml"
        return 0
    fi
}

# ÄÃ£ loáº¡i bá» deploy_ai_services_cpu - sá»­ dá»¥ng deploy_ai_services chÃ­nh

# ÄÃ£ loáº¡i bá» deploy_ai_services_gpu - sá»­ dá»¥ng deploy_ai_services chÃ­nh

# Wait for AI services
wait_for_ai_services() {
    log_info "Äá»£i cÃ¡c dá»‹ch vá»¥ AI sáºµn sÃ ng..."

    local services_to_check=(
        "ollama:60"
        "open-webui:30"
    )

    for service_timeout in "${services_to_check[@]}"; do
        local service="${service_timeout%:*}"
        local timeout="${service_timeout#*:}"

        # Check if container exists (ollama-cpu or ollama-gpu)
        if is_container_running "ollama-cpu" || is_container_running "ollama-gpu"; then
            service="ollama"
        fi

        log_loading "Kiá»ƒm tra $service..."
        if wait_for_container_health "$service" "$timeout"; then
            log_success "$service Ä‘Ã£ sáºµn sÃ ng!"
        else
            log_warning "$service cÃ³ thá»ƒ chÆ°a sáºµn sÃ ng hoÃ n toÃ n."
        fi
    done
}

# Download AI models
download_ai_models() {
    local use_gpu="$1"

    log_info "Táº£i cÃ¡c mÃ´ hÃ¬nh AI..."

    # Check if model download container is running
    local model_container=""
    if [[ "$use_gpu" == "true" ]]; then
        model_container="ollama-pull-llama-gpu"
    else
        model_container="ollama-pull-llama-cpu"
    fi

    if is_container_running "$model_container"; then
        log_info "Container táº£i mÃ´ hÃ¬nh Ä‘ang cháº¡y: $model_container"
        log_info "QuÃ¡ trÃ¬nh táº£i mÃ´ hÃ¬nh cÃ³ thá»ƒ máº¥t vÃ i phÃºt..."

        # Show progress
        local timeout=600  # 10 minutes
        local elapsed=0
        while is_container_running "$model_container" && [[ $elapsed -lt $timeout ]]; do
            show_progress $elapsed $timeout
            sleep 10
            elapsed=$((elapsed + 10))
        done

        if [[ $elapsed -ge $timeout ]]; then
            log_warning "QuÃ¡ trÃ¬nh táº£i mÃ´ hÃ¬nh máº¥t quÃ¡ lÃ¢u. Kiá»ƒm tra logs:"
            echo "  docker logs $model_container"
        else
            log_success "ÄÃ£ táº£i xong cÃ¡c mÃ´ hÃ¬nh AI!"
        fi
    else
        log_warning "Container táº£i mÃ´ hÃ¬nh khÃ´ng cháº¡y. CÃ³ thá»ƒ cáº§n táº£i mÃ´ hÃ¬nh thá»§ cÃ´ng."
        log_info "Äá»ƒ táº£i mÃ´ hÃ¬nh thá»§ cÃ´ng:"
        echo "  docker exec -it ollama ollama pull llama3.1"
        echo "  docker exec -it ollama ollama pull nomic-embed-text"
    fi
}

# Show AI summary
show_ai_summary() {
    show_banner "Tá»”NG Káº¾T TRIá»‚N KHAI AI"

    log_success "Profile AI Ä‘Ã£ Ä‘Æ°á»£c triá»ƒn khai thÃ nh cÃ´ng!"
    echo

    # Kiá»ƒm tra GPU support
    local use_gpu=false
    if docker exec ollama nvidia-smi >/dev/null 2>&1; then
        use_gpu=true
    fi

    log_info "CÃ¡c dá»‹ch vá»¥ AI Ä‘Ã£ triá»ƒn khai:"
    echo
    if [[ "$use_gpu" == "true" ]]; then
        echo "ğŸš€ AI Stack (GPU):"
        echo "  â€¢ Ollama (GPU)  : http://localhost:11434"
    else
        echo "ğŸš€ AI Stack (CPU):"
        echo "  â€¢ Ollama (CPU)  : http://localhost:11434"
    fi
    echo "  â€¢ Open WebUI    : http://localhost:5080"
    echo

    # Show Cloudflare tunnel info if running
    if is_container_running "$AI_TUNNEL_SERVICE"; then
        echo "ğŸŒ Cloudflare Tunnel:"
        echo "  â€¢ Status        : âœ… Äang cháº¡y"
        if [[ -n "${CLOUDFLARE_TUNNEL_ID:-}" ]]; then
            echo "  â€¢ Tunnel ID     : ${CLOUDFLARE_TUNNEL_ID}"
        fi
        echo "  â€¢ External URLs : Xem trong Cloudflare Dashboard"
        echo
    else
        echo "ğŸŒ Cloudflare Tunnel:"
        echo "  â€¢ Status        : âŒ ChÆ°a khá»Ÿi Ä‘á»™ng"
        echo
    fi

    log_info "HÆ°á»›ng dáº«n sá»­ dá»¥ng:"
    echo "  1. Truy cáº­p Open WebUI táº¡i http://localhost:5080"
    echo "  2. Táº¡o tÃ i khoáº£n admin (ngÆ°á»i Ä‘Äƒng kÃ½ Ä‘áº§u tiÃªn)"
    echo "  3. Chá»n mÃ´ hÃ¬nh AI tá»« danh sÃ¡ch cÃ³ sáºµn"
    echo "  4. Báº¯t Ä‘áº§u chat vá»›i AI"
    echo

    log_info "CÃ¡c mÃ´ hÃ¬nh AI cÃ³ sáºµn:"
    echo "  â€¢ llama3.1 - MÃ´ hÃ¬nh chat Ä‘a nÄƒng"
    echo "  â€¢ nomic-embed-text - MÃ´ hÃ¬nh embedding"
    echo "  â€¢ gemma2:2b - MÃ´ hÃ¬nh nháº¹"
    echo "  â€¢ deepseek-r1:7b - MÃ´ hÃ¬nh reasoning"
    echo

    log_info "Äá»ƒ táº£i thÃªm mÃ´ hÃ¬nh:"
    echo "  docker exec -it ollama ollama pull <model_name>"
    echo

    log_info "Äá»ƒ xem danh sÃ¡ch mÃ´ hÃ¬nh Ä‘Ã£ táº£i:"
    echo "  docker exec -it ollama ollama list"
    echo

    # Check service status
    log_info "Tráº¡ng thÃ¡i cÃ¡c container AI:"
    docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}" | grep -E "(ollama|open-webui)"

    # Show GPU info if available
    if [[ "$use_gpu" == "true" ]] && docker exec ollama nvidia-smi >/dev/null 2>&1; then
        echo
        log_info "ThÃ´ng tin GPU:"
        docker exec ollama nvidia-smi --query-gpu=name,memory.total,memory.used --format=csv,noheader,nounits
    fi
}

# Stop AI services
stop_ai() {
    show_banner "Dá»ªNG PROFILE AI"

    log_info "Dá»«ng cÃ¡c dá»‹ch vá»¥ AI..."

    # Stop both CPU and GPU profiles
    $DOCKER_COMPOSE -f "$COMPOSE_FILE" --profile cpu down 2>/dev/null || true
    $DOCKER_COMPOSE -f "$COMPOSE_FILE" --profile gpu-nvidia down 2>/dev/null || true

    # Stop Open WebUI
    docker stop open-webui 2>/dev/null || true

    log_success "ÄÃ£ dá»«ng táº¥t cáº£ cÃ¡c dá»‹ch vá»¥ AI!"
}

# Export functions
export -f deploy_ai stop_ai
export -f create_ai_directories deploy_ai_services deploy_ai_tunnel
export -f wait_for_ai_services show_ai_summary
