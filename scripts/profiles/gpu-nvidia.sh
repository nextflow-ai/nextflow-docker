#!/bin/bash

# ============================================================================
# NEXTFLOW DOCKER - PROFILE GPU-NVIDIA
# ============================================================================
# TÃ¡c giáº£: NextFlow Development Team
# PhiÃªn báº£n: 1.0
# NgÃ y cáº­p nháº­t: 2025-06-17
#
# MÃ” Táº¢:
# Profile nÃ y triá»ƒn khai cÃ¡c dá»‹ch vá»¥ AI vá»›i há»— trá»£ GPU NVIDIA.
# Bao gá»“m Ollama vá»›i GPU acceleration vÃ  Open-WebUI.
#
# ÄIá»€U KIá»†N TIÃŠN QUYáº¾T:
# - NVIDIA GPU driver Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t
# - NVIDIA Container Toolkit Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t
# - Docker há»— trá»£ GPU runtime
#
# Dá»ŠCH Vá»¤ TRIá»‚N KHAI:
# - Ollama (GPU-accelerated) - Cá»•ng 11434
# - Open-WebUI - Cá»•ng 5080
# ============================================================================

# Import cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
source "$SCRIPT_DIR/../utils/logging.sh"
source "$SCRIPT_DIR/../utils/docker.sh"
source "$SCRIPT_DIR/../utils/validation.sh"

# ============================================================================
# Cáº¤U HÃŒNH PROFILE GPU-NVIDIA
# ============================================================================

# TÃªn profile
PROFILE_NAME="gpu-nvidia"

# Danh sÃ¡ch services cáº§n triá»ƒn khai (service names trong docker-compose.yml)
GPU_SERVICES=(
    "ollama-gpu"
    "open-webui"
    "ollama-pull-llama-gpu"
)

# Container names tÆ°Æ¡ng á»©ng (Ä‘á»ƒ kiá»ƒm tra status)
GPU_CONTAINERS=(
    "ollama"
    "open-webui"
    "ollama-pull-llama"
)

# Cá»•ng dá»‹ch vá»¥
OLLAMA_PORT="11434"
OPEN_WEBUI_PORT="5080"

# ============================================================================
# FUNCTIONS KIá»‚M TRA GPU
# ============================================================================

# HÃ m kiá»ƒm tra NVIDIA GPU
check_nvidia_gpu() {
    log_info "ğŸ” Kiá»ƒm tra NVIDIA GPU..."
    
    # Kiá»ƒm tra nvidia-smi
    if ! command -v nvidia-smi >/dev/null 2>&1; then
        log_error "âŒ nvidia-smi khÃ´ng tÃ¬m tháº¥y. Vui lÃ²ng cÃ i Ä‘áº·t NVIDIA driver"
        return 1
    fi
    
    # Kiá»ƒm tra GPU cÃ³ sáºµn
    if ! nvidia-smi >/dev/null 2>&1; then
        log_error "âŒ KhÃ´ng thá»ƒ truy cáº­p NVIDIA GPU"
        return 1
    fi
    
    # Hiá»ƒn thá»‹ thÃ´ng tin GPU
    log_info "ğŸ“Š ThÃ´ng tin GPU:"
    nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv,noheader,nounits | while read line; do
        log_info "  GPU: $line"
    done
    
    log_success "âœ… NVIDIA GPU sáºµn sÃ ng"
    return 0
}

# HÃ m kiá»ƒm tra Docker GPU support
check_docker_gpu() {
    log_info "ğŸ” Kiá»ƒm tra Docker GPU support..."
    
    # Kiá»ƒm tra nvidia-container-runtime
    if ! docker info 2>/dev/null | grep -q "nvidia"; then
        log_warning "âš ï¸ Docker chÆ°a Ä‘Æ°á»£c cáº¥u hÃ¬nh Ä‘á»ƒ sá»­ dá»¥ng GPU"
        log_info "ğŸ’¡ HÆ°á»›ng dáº«n cÃ i Ä‘áº·t NVIDIA Container Toolkit:"
        log_info "   https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html"
    fi
    
    # Test GPU access trong Docker
    log_info "ğŸ§ª Test GPU access trong Docker..."
    if docker run --rm --gpus all nvidia/cuda:11.8-base-ubuntu20.04 nvidia-smi >/dev/null 2>&1; then
        log_success "âœ… Docker cÃ³ thá»ƒ truy cáº­p GPU"
        return 0
    else
        log_warning "âš ï¸ Docker khÃ´ng thá»ƒ truy cáº­p GPU, sáº½ cháº¡y á»Ÿ cháº¿ Ä‘á»™ CPU"
        return 1
    fi
}

# ============================================================================
# FUNCTIONS TRIá»‚N KHAI
# ============================================================================

# HÃ m kiá»ƒm tra Ä‘iá»u kiá»‡n tiÃªn quyáº¿t
check_gpu_prerequisites() {
    log_info "ğŸ” Kiá»ƒm tra Ä‘iá»u kiá»‡n tiÃªn quyáº¿t cho GPU profile..."
    
    local errors=0
    
    # 1. Kiá»ƒm tra Docker vÃ  Docker Compose
    if ! check_docker || ! check_docker_compose; then
        log_error "âŒ Docker hoáº·c Docker Compose khÃ´ng sáºµn sÃ ng"
        ((errors++))
    fi
    
    # 2. Kiá»ƒm tra NVIDIA GPU (khÃ´ng báº¯t buá»™c)
    if check_nvidia_gpu; then
        check_docker_gpu || log_warning "âš ï¸ Sáº½ cháº¡y á»Ÿ cháº¿ Ä‘á»™ CPU fallback"
    else
        log_warning "âš ï¸ KhÃ´ng tÃ¬m tháº¥y NVIDIA GPU, sáº½ cháº¡y á»Ÿ cháº¿ Ä‘á»™ CPU"
    fi
    
    # 3. Kiá»ƒm tra ports
    local ports=("$OLLAMA_PORT" "$OPEN_WEBUI_PORT")
    for port in "${ports[@]}"; do
        if is_port_in_use "$port"; then
            log_warning "âš ï¸ Cá»•ng $port Ä‘ang Ä‘Æ°á»£c sá»­ dá»¥ng"
        fi
    done
    
    # 4. Kiá»ƒm tra dung lÆ°á»£ng á»• cá»©ng (tá»‘i thiá»ƒu 10GB cho models)
    local available_space=$(df . | awk 'NR==2 {print $4}')
    local required_space=10485760  # 10GB in KB
    
    if [[ $available_space -lt $required_space ]]; then
        log_warning "âš ï¸ Dung lÆ°á»£ng á»• cá»©ng cÃ³ thá»ƒ khÃ´ng Ä‘á»§ (khuyáº¿n nghá»‹ Ã­t nháº¥t 10GB cho AI models)"
    fi
    
    if [[ $errors -gt 0 ]]; then
        log_error "âŒ TÃ¬m tháº¥y $errors lá»—i nghiÃªm trá»ng"
        return 1
    fi
    
    log_success "âœ… Äiá»u kiá»‡n tiÃªn quyáº¿t Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã¡p á»©ng"
    return 0
}

# HÃ m dá»«ng GPU services hiá»‡n táº¡i
stop_gpu_services() {
    log_info "ğŸ›‘ Dá»«ng GPU services hiá»‡n táº¡i..."

    for container in "${GPU_CONTAINERS[@]}"; do
        if is_container_running "$container"; then
            log_info "Dá»«ng container: $container"
            docker stop "$container" 2>/dev/null || true
            docker rm "$container" 2>/dev/null || true
        fi
    done

    log_success "âœ… ÄÃ£ dá»«ng táº¥t cáº£ GPU services hiá»‡n táº¡i"
}

# HÃ m triá»ƒn khai GPU services
deploy_gpu_services() {
    log_info "ğŸš€ Triá»ƒn khai GPU services..."
    
    # Pull images first
    log_info "ğŸ“¥ Táº£i GPU service images..."
    for service in "${GPU_SERVICES[@]}"; do
        log_debug "Pulling $service image..."
        $DOCKER_COMPOSE -f "$COMPOSE_FILE" pull "$service" 2>/dev/null || {
            log_warning "KhÃ´ng thá»ƒ táº£i image cho $service, sáº½ sá»­ dá»¥ng image cÃ³ sáºµn"
        }
    done
    
    # Deploy services
    log_loading "Khá»Ÿi Ä‘á»™ng GPU services..."
    for service in "${GPU_SERVICES[@]}"; do
        log_debug "Starting $service..."
        $DOCKER_COMPOSE -f "$COMPOSE_FILE" up -d "$service" || {
            log_warning "KhÃ´ng thá»ƒ khá»Ÿi Ä‘á»™ng $service"
        }
    done
    
    log_success "âœ… ÄÃ£ triá»ƒn khai GPU services!"
}

# HÃ m kiá»ƒm tra tráº¡ng thÃ¡i GPU services
check_gpu_status() {
    log_info "ğŸ“Š Kiá»ƒm tra tráº¡ng thÃ¡i GPU services..."

    local all_healthy=true

    for container in "${GPU_CONTAINERS[@]}"; do
        if is_container_running "$container"; then
            log_success "âœ… $container: Äang cháº¡y"

            # Kiá»ƒm tra health cá»¥ thá»ƒ
            case "$container" in
                "ollama")
                    if curl -s http://localhost:$OLLAMA_PORT/api/tags >/dev/null 2>&1; then
                        log_success "âœ… Ollama API: Hoáº¡t Ä‘á»™ng"
                    else
                        log_warning "âš ï¸ Ollama API: ChÆ°a sáºµn sÃ ng"
                    fi
                    ;;
                "open-webui")
                    if curl -s http://localhost:$OPEN_WEBUI_PORT >/dev/null 2>&1; then
                        log_success "âœ… Open-WebUI: Hoáº¡t Ä‘á»™ng"
                    else
                        log_warning "âš ï¸ Open-WebUI: ChÆ°a sáºµn sÃ ng"
                    fi
                    ;;
                "ollama-pull-llama")
                    log_info "â„¹ï¸ $container: Init container (cháº¡y má»™t láº§n)"
                    ;;
            esac
        else
            log_error "âŒ $container: KhÃ´ng cháº¡y"
            all_healthy=false
        fi
    done

    if [[ "$all_healthy" == "true" ]]; then
        log_success "ğŸ‰ Táº¥t cáº£ GPU services hoáº¡t Ä‘á»™ng bÃ¬nh thÆ°á»ng!"
        return 0
    else
        log_error "âŒ Má»™t sá»‘ GPU services cÃ³ váº¥n Ä‘á»"
        return 1
    fi
}

# HÃ m hiá»ƒn thá»‹ thÃ´ng tin truy cáº­p
show_gpu_access_info() {
    log_info "ğŸŒ ThÃ´ng tin truy cáº­p GPU services:"
    echo
    echo "ğŸ¤– Ollama (AI Model Server):"
    echo "   ğŸ”— API Endpoint: http://localhost:${OLLAMA_PORT}"
    echo "   ğŸ“‹ List Models: curl http://localhost:${OLLAMA_PORT}/api/tags"
    echo "   ğŸ’¬ Chat API: http://localhost:${OLLAMA_PORT}/api/chat"
    echo
    echo "ğŸŒ Open-WebUI (Chat Interface):"
    echo "   ğŸ”— Web Interface: http://localhost:${OPEN_WEBUI_PORT}"
    echo "   ğŸ‘¤ ÄÄƒng kÃ½ tÃ i khoáº£n Ä‘áº§u tiÃªn sáº½ trá»Ÿ thÃ nh admin"
    echo
    echo "ğŸ”§ Lá»‡nh quáº£n lÃ½ há»¯u Ã­ch:"
    echo "   ğŸ“‹ Xem models: docker exec ollama ollama list"
    echo "   ğŸ“¥ Táº£i model: docker exec ollama ollama pull llama2"
    echo "   ğŸ’¬ Chat CLI: docker exec -it ollama ollama run llama2"
    echo "   ğŸ“ Xem logs: docker logs ollama"
    echo
    echo "ğŸ¯ Models khuyáº¿n nghá»‹:"
    echo "   â€¢ llama2:7b (4GB VRAM)"
    echo "   â€¢ codellama:7b (4GB VRAM)"
    echo "   â€¢ mistral:7b (4GB VRAM)"
    echo "   â€¢ llama2:13b (8GB VRAM)"
    echo
}

# ============================================================================
# FUNCTION CHÃNH - TRIá»‚N KHAI GPU PROFILE
# ============================================================================

# HÃ m chÃ­nh triá»ƒn khai gpu-nvidia profile
deploy_gpu_nvidia() {
    local force_deploy="${1:-false}"
    
    show_banner "ğŸ® TRIá»‚N KHAI GPU-NVIDIA PROFILE"
    
    log_info "ğŸš€ Báº¯t Ä‘áº§u triá»ƒn khai GPU services vá»›i NVIDIA acceleration..."
    
    # 1. Kiá»ƒm tra Ä‘iá»u kiá»‡n tiÃªn quyáº¿t
    if ! check_gpu_prerequisites; then
        if [[ "$force_deploy" != "true" ]]; then
            log_error "âŒ Äiá»u kiá»‡n tiÃªn quyáº¿t khÃ´ng Ä‘Æ°á»£c Ä‘Ã¡p á»©ng"
            return 1
        else
            log_warning "âš ï¸ Bá» qua lá»—i Ä‘iá»u kiá»‡n tiÃªn quyáº¿t (force deploy)"
        fi
    fi
    
    # 2. Dá»«ng GPU services hiá»‡n táº¡i (chá»‰ GPU services)
    stop_gpu_services
    
    # 3. Triá»ƒn khai GPU services
    if ! deploy_gpu_services; then
        log_error "âŒ Triá»ƒn khai GPU services tháº¥t báº¡i"
        return 1
    fi
    
    # 4. Äá»£i services khá»Ÿi Ä‘á»™ng
    log_info "â³ Äá»£i GPU services khá»Ÿi Ä‘á»™ng..."
    sleep 15
    
    # 5. Kiá»ƒm tra tráº¡ng thÃ¡i cuá»‘i cÃ¹ng
    if check_gpu_status; then
        log_success "ğŸ‰ Triá»ƒn khai GPU-NVIDIA profile hoÃ n táº¥t thÃ nh cÃ´ng!"
        show_gpu_access_info
        return 0
    else
        log_error "âŒ Triá»ƒn khai cÃ³ má»™t sá»‘ váº¥n Ä‘á»"
        log_info "ğŸ’¡ Kiá»ƒm tra logs Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t:"
        log_info "   docker logs ollama"
        log_info "   docker logs open-webui"
        log_info "   docker logs ollama-pull-llama"
        return 1
    fi
}

# ============================================================================
# EXPORT FUNCTIONS
# ============================================================================

# Export cÃ¡c functions Ä‘á»ƒ cÃ³ thá»ƒ sá»­ dá»¥ng tá»« bÃªn ngoÃ i
export -f deploy_gpu_nvidia
export -f check_gpu_status
export -f show_gpu_access_info
